{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from datetime import timedelta \n",
    "import dateparser as dp\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyts.image import GramianAngularField\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from pyts.image import MarkovTransitionField\n",
    "from pyts.image import RecurrencePlot\n",
    "import io\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import gc\n",
    "import psutil\n",
    "import pickle\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "import random\n",
    "from keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = <s3_bucket_name>\n",
    "DATA_POINTS_PER_WINDOW = 21\n",
    "s3Res = boto3.resource('s3')\n",
    "bucket = s3Res.Bucket(BUCKET_NAME)\n",
    "labelledDataCommonPath = <path_to_labelleddata_dir_on_s3>\n",
    "tempDiskSaveLoc = '<path>/tmpImage.png'\n",
    "s3Client = boto3.client('s3')\n",
    "INPUT_MATRIX_WIDTH = 21\n",
    "ENCODED_FEATURES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGAFMatrix(df, feature, index, method='summation', span=10):\n",
    "    X = [df.loc[(index - timedelta(minutes=span)) : (index + timedelta(minutes=span)), feature]]\n",
    "    \n",
    "    if len(X[0]) != DATA_POINTS_PER_WINDOW:\n",
    "        print(\"GAF error..{} Length != {}, {} point={}\".format(feature, DATA_POINTS_PER_WINDOW, len(X[0]), index))\n",
    "        raise Exception('GAF Length != %d, %d' %(DATA_POINTS_PER_WINDOW, len(X[0]))) \n",
    "    \n",
    "    gaf = GramianAngularField(method = method, overlapping = False)\n",
    "    x_gaf = gaf.fit_transform(X)\n",
    "    return x_gaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket = s3Res.Bucket(BUCKET_NAME)\n",
    "\n",
    "csvFileList = []\n",
    "\n",
    "for my_bucket_object in my_bucket.objects.filter(Prefix=labelledDataCommonPath):\n",
    "    if '.csv' in my_bucket_object.key:\n",
    "        print(my_bucket_object.key)\n",
    "        csvFileList.append(my_bucket_object.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSurroundingIndexesToPosIndex(posIndexes, df):\n",
    "    surroundIndexesToPosIndexes = pd.Index([])\n",
    "    \n",
    "    for pos in posIndexes:\n",
    "        sampleIndex = df.loc[(pos - timedelta(minutes=10)) : (pos + timedelta(minutes=10))].sample(1).index\n",
    "        surroundIndexesToPosIndexes = surroundIndexesToPosIndexes.union(sampleIndex)\n",
    "    \n",
    "    return surroundIndexesToPosIndexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fileIndex = 0\n",
    "\n",
    "encodedFeatures = ['Price', 'Volume']\n",
    "encoded_feature_count = len(encodedFeatures)\n",
    "minVicinity = 20\n",
    "\n",
    "X_data=[]\n",
    "Y_data=[]\n",
    "\n",
    "NUMBER_OF_FILES_USEDTO_TRAIN = 20\n",
    "\n",
    "for file_name in csvFileList[:NUMBER_OF_FILES_USEDTO_TRAIN]:\n",
    "    print(file_name)\n",
    "    \n",
    "    coin_name = file_name.split('/')[-1].split('_')[1]\n",
    "    fileIndex +=1\n",
    "    obj = s3Client.get_object(Bucket = BUCKET_NAME, Key = file_name)\n",
    "    df = pd.read_csv(obj['Body'], index_col='0', parse_dates=True)\n",
    "    \n",
    "    anomalyIndexes = df[df.Label==1].index\n",
    "        \n",
    "    for i in anomalyIndexes:\n",
    "        mat = np.zeros((DATA_POINTS_PER_WINDOW, DATA_POINTS_PER_WINDOW, encoded_feature_count), 'float32')    \n",
    "        try:\n",
    "            price = getGAFMatrix(df, 'Price', i, method='summation', span=10)\n",
    "            vol = getGAFMatrix(df, 'Volume', i, method='summation', span=10)\n",
    "        except:\n",
    "            print(\"Anomaly case={} exception occurred for coin when GASF {}\".format(i.strftime('%Y-%m-%d_%H%M%S'), coin_name))\n",
    "            continue\n",
    "        \n",
    "        mat[:,:,0]=price[0]\n",
    "        mat[:,:,1]=vol[0]\n",
    "        y=1\n",
    "        X_data.append(mat)\n",
    "        Y_data.append(y)\n",
    "    \n",
    "    if ( len(df[df.Label==0].index) > int(len(anomalyIndexes)/2) ):\n",
    "        nonAnomalousIndexes = df[df.Label==0].sample(int(len(anomalyIndexes)/2), random_state=79).index\n",
    "    else:\n",
    "#         take a half from non anomalous indexes\n",
    "        nonAnomalousIndexes = df[df.Label==0].sample(int(len(df[df.Label==0])/2), random_state=79).index\n",
    "            \n",
    "    surroundingIndexesToPosIndexes = getSurroundingIndexesToPosIndex(anomalyIndexes, df)\n",
    "    nonAnomalousIndexes = nonAnomalousIndexes.union(surroundingIndexesToPosIndexes[:int(len(surroundingIndexesToPosIndexes)/2)])\n",
    "    \n",
    "    print(\"number of non anom cases={}\".format(len(nonAnomalousIndexes)))\n",
    "    print(\"number of non anom cases={}\".format(len(anomalyIndexes)))\n",
    "    \n",
    "    for i in nonAnomalousIndexes:        \n",
    "        mat = np.zeros((DATA_POINTS_PER_WINDOW, DATA_POINTS_PER_WINDOW, encoded_feature_count), 'float32')\n",
    "        \n",
    "        try:\n",
    "            price = getGAFMatrix(df, 'Price', i, method='summation', span=10)\n",
    "            vol = getGAFMatrix(df, 'Volume', i, method='summation', span=10)\n",
    "        except:\n",
    "            print(\"NonAnomaly case={} exception occurred for coin when GASF {}\".format(i.strftime('%Y-%m-%d_%H%M%S'), coin_name))\n",
    "            continue\n",
    "        \n",
    "        mat[:,:,0]=price[0]\n",
    "        mat[:,:,1]=vol[0]\n",
    "        y=0\n",
    "        X_data.append(mat)\n",
    "        Y_data.append(y)\n",
    "    \n",
    "    print('-------------- processed files %d' %fileIndex)\n",
    "    print(psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dataArr = np.array(Y_data)\n",
    "X_dataArr = np.array(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=Y_dataArr, palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gadfcnn5_model():    \n",
    "    cnn=Sequential()\n",
    "    cnn.add(Conv2D(filters=64, kernel_size=(2,2), padding='same', activation='relu', input_shape=(INPUT_MATRIX_WIDTH, INPUT_MATRIX_WIDTH, ENCODED_FEATURES)))\n",
    "    cnn.add(Conv2D(filters=64, kernel_size=(2,2), padding='same', activation='relu'))\n",
    "    cnn.add(Dropout(0.25))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(256, activation='relu'))\n",
    "    cnn.add(Dropout(0.5))\n",
    "    cnn.add(Dense(1, activation='sigmoid'))\n",
    "    cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10)\n",
    "history = []\n",
    "confusions= []\n",
    "classifReports= []\n",
    "\n",
    "fold = 0\n",
    "\n",
    "for train, test in kf.split(X_dataArr, Y_dataArr):\n",
    "    print('Running fold [%d]'.ljust(100,'*') %fold)\n",
    "    fold +=1\n",
    "    \n",
    "    cnn=create_gadfcnn5_model()\n",
    "    \n",
    "    x_train, x_test = X_dataArr[train], X_dataArr[test]\n",
    "    y_train, y_test = Y_dataArr[train], Y_dataArr[test]\n",
    "    \n",
    "    hist = cnn.fit(x=x_train, y=y_train, validation_split=0.2, epochs=20, batch_size=500, verbose=0)\n",
    "    history.append(hist)\n",
    "    \n",
    "    y_pred = cnn.predict(x_test)\n",
    "\n",
    "    y_pred_R = np.round(y_pred)\n",
    "    conf = confusion_matrix(y_test, y_pred_R)\n",
    "    confusions.append(conf)\n",
    "    \n",
    "    clfr = classification_report(y_test, y_pred_R, output_dict=True)\n",
    "    print(clfr)\n",
    "    classifReports.append(clfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "f1s = [rep['macro avg']['f1-score'] for rep in classifReports]\n",
    "recalls = [rep['macro avg']['recall'] for rep in classifReports]\n",
    "precisions = [rep['macro avg']['precision'] for rep in classifReports]\n",
    "\n",
    "print(statistics.variance(f1s))\n",
    "print(statistics.variance(recalls))\n",
    "print(statistics.variance(precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.stdev(f1s))\n",
    "print(statistics.stdev(recalls))\n",
    "print(statistics.stdev(precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y_pred_R==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=9\n",
    "plt.plot(history[j].history['acc'])\n",
    "plt.plot(history[j].history['val_acc'])\n",
    "\n",
    "plt.legend(['acc','val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=9\n",
    "plt.plot(history[j].history['loss'])\n",
    "plt.plot(history[j].history['val_loss'])\n",
    "plt.legend(['loss','val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "finConf=np.zeros((2,2), dtype=int)\n",
    "for elem in confusions:\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "             finConf[i][j] += elem[i][j]\n",
    "                \n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(finConf/np.sum(finConf), annot=True, fmt='.2%', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "macroPrec=[]\n",
    "macroRecall=[]\n",
    "macrof1=[]\n",
    "\n",
    "for elem in classifReports:\n",
    "    macroPrec.append(elem['macro avg']['precision'])\n",
    "    macroRecall.append(elem['macro avg']['recall'])\n",
    "    macrof1.append(elem['macro avg']['f1-score'])\n",
    "    \n",
    "print(np.mean(macroPrec))\n",
    "print(np.mean(macroRecall))\n",
    "print(np.mean(macrof1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighPrec=[]\n",
    "weighRecall=[]\n",
    "weighf1=[]\n",
    "\n",
    "for elem in classifReports:\n",
    "    weighPrec.append(elem['weighted avg']['precision'])\n",
    "    weighRecall.append(elem['weighted avg']['recall'])\n",
    "    weighf1.append(elem['weighted avg']['f1-score'])\n",
    "    \n",
    "print(np.mean(weighPrec))\n",
    "print(np.mean(weighRecall))\n",
    "print(np.mean(weighf1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do a model train using X and Y created from first 20 files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadfCnn5 = create_gadfcnn5_model()\n",
    "hist = gadfCnn5.fit(x=X_dataArr, y=Y_dataArr, validation_split=0.2, epochs=10, batch_size=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.legend(['acc','val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.legend(['loss','val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSaveLocOnDisk = '<path>/GASF_CNN5.h5'\n",
    "gadfCnn5.save(modelSaveLocOnDisk)\n",
    "print(\"Saved model to disk at {}\".format(modelSaveLocOnDisk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(modelSaveLocOnDisk)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_dataArr.shape)\n",
    "print(Y_dataArr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "score = model.evaluate(X_dataArr, Y_dataArr, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
