{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from datetime import timedelta \n",
    "import dateparser as dp\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyts.image import GramianAngularField\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from pyts.image import MarkovTransitionField\n",
    "from PIL import Image\n",
    "from pyts.image import RecurrencePlot\n",
    "import io\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import gc\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyLabelGenerator:\n",
    "    \n",
    "    def isConsistentIncrease(self, x):\n",
    "        isConsisIncrease = True\n",
    "        i=0\n",
    "        while ((i < len(x)) and (isConsisIncrease)):\n",
    "            if (i+1 < len(x)):\n",
    "                isConsisIncrease = isConsisIncrease & (x.iloc[i] <= x.iloc[i+1])\n",
    "            i += 1\n",
    "        return isConsisIncrease\n",
    "    \n",
    "\n",
    "    def isOpenToClosePercentageBeyondThreshold(self, x, percentage):\n",
    "        return (((x.iloc[-1] - x.iloc[0])/x.iloc[0]*100) >= percentage)\n",
    "    \n",
    "\n",
    "    def isWindowClosingTurnoverSignificant(self, x, percentage):\n",
    "        cumSumUptoClose = x.iloc[:-1].sum()\n",
    "        if ((cumSumUptoClose*percentage/100) <= x.iloc[-1]):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def isWidowsMaxSignificant(self, x, percentage):\n",
    "        sumExceptMax = x.sum() - x.max()\n",
    "        if sumExceptMax*percentage/100 <= x.max():\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def isNextValBelowThreshold(self, x, percentage):\n",
    "        return ((x.iloc[0]*percentage/100) > x.iloc[1])\n",
    "    \n",
    "    def isLeftWindowPumping(self, x):\n",
    "        return (x.iloc[0] < x.iloc[int(len(x)/2)] < x.iloc[-1])\n",
    "    \n",
    "    def isRightWindowDumping(self, x):\n",
    "        return ( (x.iloc[0] > x.iloc[int(len(x)/2)]) and (x.iloc[0] > x.iloc[-1]) )\n",
    "\n",
    "labelGen = AnomalyLabelGenerator();\n",
    "TURNOVER_ROLLING_WINDOW_SIZE = '5Min'\n",
    "TURNOVER_MIN_POINTS=5\n",
    "PRICE_ROLLING_WINDOW_SIZE = '10Min'\n",
    "PRICE_MIN_POINTS=10\n",
    "OPEN_TO_CLOSE_TURNOVER_INCREASE_PERCENTAGE = 75\n",
    "PRICE_DROP_PERCENTAGE = 100\n",
    "\n",
    "BUCKET_NAME = <s3_bucket_name>\n",
    "dataLocationCommonPath = <path_to_download_dir_on_s3>\n",
    "destS3Url = <s3_url>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertNewFeaturesToDf(filename, df, featureStats=None):\n",
    "    t1 = time.time()\n",
    "        \n",
    "    featureStats.write('*' *100)\n",
    "    featureStats.write('\\nAdding features to df for file [%s]\\n' %filename)\n",
    "    \n",
    "    df['Price'] = (df['Open'] + df['High'] + df['Low'] + df['Close'])/4\n",
    "    df['Turnover'] = df['Price'] * df['Volume']\n",
    "    df['RollingPrice'] = df['Price'].rolling(PRICE_ROLLING_WINDOW_SIZE, center=False).mean()\n",
    "    \n",
    "    df['isOpenToClosePriceAbove10P'] = df['Price'].rolling(PRICE_ROLLING_WINDOW_SIZE, center=False, min_periods=PRICE_MIN_POINTS).apply(labelGen.isOpenToClosePercentageBeyondThreshold, kwargs={'percentage':10}).fillna(0)\n",
    "    featureStats.write(df.groupby('isOpenToClosePriceAbove10P')['isOpenToClosePriceAbove10P'].agg(['count']).to_string())\n",
    "    featureStats.write('\\n')\n",
    "    \n",
    "    df['isOpenToClosePriceAbove15P'] = df['Price'].rolling(PRICE_ROLLING_WINDOW_SIZE, center=False, min_periods=PRICE_MIN_POINTS).apply(labelGen.isOpenToClosePercentageBeyondThreshold, kwargs={'percentage':15}).fillna(0)\n",
    "    featureStats.write(df.groupby('isOpenToClosePriceAbove15P')['isOpenToClosePriceAbove15P'].agg(['count']).to_string())\n",
    "    featureStats.write('\\n')\n",
    "    \n",
    "    df['isOpenToClosePriceAbove20P'] = df['Price'].rolling(PRICE_ROLLING_WINDOW_SIZE, center=False, min_periods=PRICE_MIN_POINTS).apply(labelGen.isOpenToClosePercentageBeyondThreshold, kwargs={'percentage':20}).fillna(0)\n",
    "    featureStats.write(df.groupby('isOpenToClosePriceAbove20P')['isOpenToClosePriceAbove20P'].agg(['count']).to_string())\n",
    "    featureStats.write('\\n')\n",
    "    \n",
    "    df['isWindowClosingTurnoverSignificant'] = df['Turnover'].rolling(TURNOVER_ROLLING_WINDOW_SIZE, center=False, min_periods=TURNOVER_MIN_POINTS).apply(labelGen.isWindowClosingTurnoverSignificant, kwargs={'percentage':OPEN_TO_CLOSE_TURNOVER_INCREASE_PERCENTAGE}).fillna(0)\n",
    "    featureStats.write(df.groupby('isWindowClosingTurnoverSignificant')['isWindowClosingTurnoverSignificant'].agg(['count']).to_string())\n",
    "    featureStats.write('\\n')\n",
    "    \n",
    "    df['isLeftWindowPumping'] = df['Price'].rolling(PRICE_ROLLING_WINDOW_SIZE, center=False, min_periods=PRICE_MIN_POINTS).apply(labelGen.isLeftWindowPumping).fillna(0)\n",
    "    featureStats.write(df.groupby('isLeftWindowPumping')['isLeftWindowPumping'].agg(['count']).to_string())\n",
    "    featureStats.write('\\n')\n",
    "    \n",
    "    df['isRightWindowDumping'] = df['Price'].shift(-10).rolling(PRICE_ROLLING_WINDOW_SIZE, center=False, min_periods=PRICE_MIN_POINTS).apply(labelGen.isRightWindowDumping).fillna(0)\n",
    "    featureStats.write(df.groupby('isRightWindowDumping')['isRightWindowDumping'].agg(['count']).to_string())\n",
    "    featureStats.write('\\n')\n",
    "    \n",
    "    featureStats.write('\\nElapsed time to insert Features={}\\n'.format(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(BUCKET_NAME)\n",
    "\n",
    "csvFileList = []\n",
    "\n",
    "for my_bucket_object in my_bucket.objects.filter(Prefix='{}/OriginalData/'.format(dataLocationCommonPath)):\n",
    "    if '.csv' in my_bucket_object.key:\n",
    "        print(my_bucket_object.key)\n",
    "        csvFileList.append(my_bucket_object.key)\n",
    "        \n",
    "print(\"Total number of loaded csv files=[%d]\" %len(csvFileList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Enrichment main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "featureStats = open('TwoYearsDataEnrichment_10P_15P_20P_Pump.txt','w')\n",
    "fileIndex = 0\n",
    "\n",
    "startTime = time.time()\n",
    "s3 = boto3.client('s3') \n",
    "\n",
    "for file_name in tqdm(csvFileList):\n",
    "    print('Processing file [%s]' %file_name)\n",
    "    coin_name = file_name.split('/')[-1].split('_')[0]\n",
    "    \n",
    "    fileIndex +=1\n",
    "    \n",
    "    obj = s3.get_object(Bucket = BUCKET_NAME, Key = file_name)\n",
    "    df = pd.read_csv(obj['Body'], index_col='0', parse_dates=True)\n",
    "    \n",
    "    df.drop(['Close Time', 'Quote Asset Volume', 'Taker buy base asset volume', 'Taker buy quote asset volume', 'Ignore'], axis=1, inplace=True)\n",
    "    \n",
    "    insertNewFeaturesToDf(file_name, df, featureStats)\n",
    "    \n",
    "    df.to_csv(destS3Url + \"Enriched_\" + file_name.split('/')[-1] , index=True)    \n",
    "\n",
    "featureStats.write('\\nTotal elapsed time to insert all features={}\\n'.format(time.time()-startTime))\n",
    "featureStats.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
